{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Simple Gen AI APP Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x20ed7e09cc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://langchain-ai.github.io/langgraph/concepts/#langgraph\")\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConcepts\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n              Concepts\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n  \\n    \\n  \\n  Home\\n\\n      \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Tutorials\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  How-to Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Conceptual Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Home\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Tutorials\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How-to Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Conceptual Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    Conceptual Guides\\n  \\n\\n          \\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Why LangGraph?\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Glossary\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Agent architectures\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Multi-agent Systems\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Human-in-the-loop\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Persistence\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Memory\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Streaming\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    FAQ\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Platform\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph Platform\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph Platform\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    High Level\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Components\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Server\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Deployment Options\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        LangGraph\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        LangGraph Platform\\n      \\n    \\n\\n\\n\\n\\n\\n\\n      \\n        High Level\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Components\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        LangGraph Server\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Deployment Options\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n    Conceptual Guides\\n  \\n\\n\\n\\n\\n\\n\\n\\nConceptual Guide¶\\nThis guide provides explanations of the key concepts behind the LangGraph framework and AI applications more broadly.\\nWe recommend that you go through at least the Quick Start before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the Tutorials and How-to guides. For detailed reference material, please see the API reference.\\nLangGraph¶\\nHigh Level\\n\\nWhy LangGraph?: A high-level overview of LangGraph and its goals.\\n\\nConcepts\\n\\nLangGraph Glossary: LangGraph workflows are designed as graphs, with nodes representing different components and edges representing the flow of information between them. This guide provides an overview of the key concepts associated with LangGraph graph primitives.\\nCommon Agentic Patterns: An agent uses an LLM to pick its own control flow to solve more complex problems! Agents are a key building block in many LLM applications. This guide explains the different types of agent architectures and how they can be used to control the flow of an application.\\nMulti-Agent Systems: Complex LLM applications can often be broken down into multiple agents, each responsible for a different part of the application. This guide explains common patterns for building multi-agent systems.\\nHuman-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.\\nPersistence: LangGraph has a built-in persistence layer, implemented through checkpointers. This persistence layer helps to support powerful capabilities like human-in-the-loop, memory, time travel, and fault-tolerance.\\nMemory: Memory in AI applications refers to the ability to process, store, and effectively recall information from past interactions. With memory, your agents can learn from feedback and adapt to users' preferences.  \\nStreaming: Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs. \\nFAQ: Frequently asked questions about LangGraph.\\n\\nLangGraph Platform¶\\nLangGraph Platform is a commercial solution for deploying agentic applications in production, built on the open-source LangGraph framework.\\nThe LangGraph Platform offers a few different deployment options described in the deployment options guide.\\n\\nTip\\n\\nLangGraph is an MIT-licensed open-source library, which we are committed to maintaining and growing for the community.\\nYou can always deploy LangGraph applications on your own infrastructure using the open-source LangGraph project without using LangGraph Platform.\\n\\n\\nHigh Level¶\\n\\nWhy LangGraph Platform?: The LangGraph platform is an opinionated way to deploy and manage LangGraph applications. This guide provides an overview of the key features and concepts behind LangGraph Platform.\\nDeployment Options: LangGraph Platform offers four deployment options: Self-Hosted Lite, Self-Hosted Enterprise, bring your own cloud (BYOC), and Cloud SaaS. This guide explains the differences between these options, and which Plans they are available on.\\nPlans: LangGraph Platforms offer three different plans: Developer, Plus, Enterprise. This guide explains the differences between these options, what deployment options are available for each, and how to sign up for each one.\\nTemplate Applications: Reference applications designed to help you get started quickly when building with LangGraph.\\n\\nComponents¶\\nThe LangGraph Platform comprises several components that work together to support the deployment and management of LangGraph applications:\\n\\nLangGraph Server: The LangGraph Server is designed to support a wide range of agentic application use cases, from background processing to real-time interactions. \\nLangGraph Studio: LangGraph Studio is a specialized IDE that can connect to a LangGraph Server to enable visualization, interaction, and debugging of the application locally.\\nLangGraph CLI: LangGraph CLI is a command-line interface that helps to interact with a local LangGraph\\nPython/JS SDK: The Python/JS SDK provides a programmatic way to interact with deployed LangGraph Applications.\\nRemote Graph: A RemoteGraph allows you to interact with any deployed LangGraph application as though it were running locally.\\n\\nLangGraph Server¶\\n\\nApplication Structure: A LangGraph application consists of one or more graphs, a LangGraph API Configuration file (langgraph.json), a file that specifies dependencies, and environment variables.\\nAssistants: Assistants are a way to save and manage different configurations of your LangGraph applications.\\nWeb-hooks: Webhooks allow your running LangGraph application to send data to external services on specific events.\\nCron Jobs: Cron jobs are a way to schedule tasks to run at specific times in your LangGraph application.\\nDouble Texting: Double texting is a common issue in LLM applications where users may send multiple messages before the graph has finished running. This guide explains how to handle double texting with LangGraph Deploy.\\n\\nDeployment Options¶\\n\\nSelf-Hosted Lite: A free (up to 1 million nodes executed), limited version of LangGraph Platform that you can run locally or in a self-hosted manner\\nCloud SaaS: Hosted as part of LangSmith.\\nBring Your Own Cloud: We manage the infrastructure, so you don't have to, but the infrastructure all runs within your cloud.\\nSelf-Hosted Enterprise: Completely managed by you.\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                MULTIPLE_SUBGRAPHS\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Why LangGraph?\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='Concepts\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n              Concepts\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n  \\n    \\n  \\n  Home\\n\\n      \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Tutorials\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  How-to Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Conceptual Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Home\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Tutorials\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    How-to Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Conceptual Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    Conceptual Guides\\n  \\n\\n          \\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Why LangGraph?\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Glossary'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Why LangGraph?\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Glossary\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Agent architectures\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Multi-agent Systems\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Human-in-the-loop\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Persistence\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Memory\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Streaming\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    FAQ\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Platform\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    LangGraph Platform\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    LangGraph Platform\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    High Level\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Components\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Server\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Deployment Options\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        LangGraph\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        LangGraph Platform'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='Reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        LangGraph\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        LangGraph Platform\\n      \\n    \\n\\n\\n\\n\\n\\n\\n      \\n        High Level\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Components\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        LangGraph Server\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Deployment Options\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n    Conceptual Guides'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Server\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Deployment Options\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n\\n\\n\\n\\n\\n    Conceptual Guides\\n  \\n\\n\\n\\n\\n\\n\\n\\nConceptual Guide¶\\nThis guide provides explanations of the key concepts behind the LangGraph framework and AI applications more broadly.\\nWe recommend that you go through at least the Quick Start before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the Tutorials and How-to guides. For detailed reference material, please see the API reference.\\nLangGraph¶\\nHigh Level\\n\\nWhy LangGraph?: A high-level overview of LangGraph and its goals.\\n\\nConcepts'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Glossary: LangGraph workflows are designed as graphs, with nodes representing different components and edges representing the flow of information between them. This guide provides an overview of the key concepts associated with LangGraph graph primitives.\\nCommon Agentic Patterns: An agent uses an LLM to pick its own control flow to solve more complex problems! Agents are a key building block in many LLM applications. This guide explains the different types of agent architectures and how they can be used to control the flow of an application.\\nMulti-Agent Systems: Complex LLM applications can often be broken down into multiple agents, each responsible for a different part of the application. This guide explains common patterns for building multi-agent systems.\\nHuman-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content=\"Human-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.\\nPersistence: LangGraph has a built-in persistence layer, implemented through checkpointers. This persistence layer helps to support powerful capabilities like human-in-the-loop, memory, time travel, and fault-tolerance.\\nMemory: Memory in AI applications refers to the ability to process, store, and effectively recall information from past interactions. With memory, your agents can learn from feedback and adapt to users' preferences.  \\nStreaming: Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs. \\nFAQ: Frequently asked questions about LangGraph.\"),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Platform¶\\nLangGraph Platform is a commercial solution for deploying agentic applications in production, built on the open-source LangGraph framework.\\nThe LangGraph Platform offers a few different deployment options described in the deployment options guide.\\n\\nTip\\n\\nLangGraph is an MIT-licensed open-source library, which we are committed to maintaining and growing for the community.\\nYou can always deploy LangGraph applications on your own infrastructure using the open-source LangGraph project without using LangGraph Platform.\\n\\n\\nHigh Level¶'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='High Level¶\\n\\nWhy LangGraph Platform?: The LangGraph platform is an opinionated way to deploy and manage LangGraph applications. This guide provides an overview of the key features and concepts behind LangGraph Platform.\\nDeployment Options: LangGraph Platform offers four deployment options: Self-Hosted Lite, Self-Hosted Enterprise, bring your own cloud (BYOC), and Cloud SaaS. This guide explains the differences between these options, and which Plans they are available on.\\nPlans: LangGraph Platforms offer three different plans: Developer, Plus, Enterprise. This guide explains the differences between these options, what deployment options are available for each, and how to sign up for each one.\\nTemplate Applications: Reference applications designed to help you get started quickly when building with LangGraph.\\n\\nComponents¶\\nThe LangGraph Platform comprises several components that work together to support the deployment and management of LangGraph applications:'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='Components¶\\nThe LangGraph Platform comprises several components that work together to support the deployment and management of LangGraph applications:\\n\\nLangGraph Server: The LangGraph Server is designed to support a wide range of agentic application use cases, from background processing to real-time interactions. \\nLangGraph Studio: LangGraph Studio is a specialized IDE that can connect to a LangGraph Server to enable visualization, interaction, and debugging of the application locally.\\nLangGraph CLI: LangGraph CLI is a command-line interface that helps to interact with a local LangGraph\\nPython/JS SDK: The Python/JS SDK provides a programmatic way to interact with deployed LangGraph Applications.\\nRemote Graph: A RemoteGraph allows you to interact with any deployed LangGraph application as though it were running locally.\\n\\nLangGraph Server¶'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Server¶\\n\\nApplication Structure: A LangGraph application consists of one or more graphs, a LangGraph API Configuration file (langgraph.json), a file that specifies dependencies, and environment variables.\\nAssistants: Assistants are a way to save and manage different configurations of your LangGraph applications.\\nWeb-hooks: Webhooks allow your running LangGraph application to send data to external services on specific events.\\nCron Jobs: Cron jobs are a way to schedule tasks to run at specific times in your LangGraph application.\\nDouble Texting: Double texting is a common issue in LLM applications where users may send multiple messages before the graph has finished running. This guide explains how to handle double texting with LangGraph Deploy.\\n\\nDeployment Options¶'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content=\"Deployment Options¶\\n\\nSelf-Hosted Lite: A free (up to 1 million nodes executed), limited version of LangGraph Platform that you can run locally or in a self-hosted manner\\nCloud SaaS: Hosted as part of LangSmith.\\nBring Your Own Cloud: We manage the infrastructure, so you don't have to, but the infrastructure all runs within your cloud.\\nSelf-Hosted Enterprise: Completely managed by you.\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                MULTIPLE_SUBGRAPHS\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Why LangGraph?\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x20ef02a9630>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Deployment Options¶\\n\\nSelf-Hosted Lite: A free (up to 1 million nodes executed), limited version of LangGraph Platform that you can run locally or in a self-hosted manner\\nCloud SaaS: Hosted as part of LangSmith.\\nBring Your Own Cloud: We manage the infrastructure, so you don't have to, but the infrastructure all runs within your cloud.\\nSelf-Hosted Enterprise: Completely managed by you.\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                MULTIPLE_SUBGRAPHS\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Why LangGraph?\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"LangSmith has two usage limits: total traces and extended\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000020EF02AA8F0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020ED840A470>, root_client=<openai.OpenAI object at 0x0000020EF02AAAA0>, root_async_client=<openai.AsyncOpenAI object at 0x0000020EF02AA320>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith has two usage limits: total traces and extended traces. These limits correspond to the two metrics tracked on the usage graph.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x20ef02a9630>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020EF02A9630>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000020EF02AA8F0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020ED840A470>, root_client=<openai.OpenAI object at 0x0000020EF02AAAA0>, root_async_client=<openai.AsyncOpenAI object at 0x0000020EF02AA320>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the deployment options mentioned for LangGraph Platform based on the provided context?\\n\\nThe deployment options mentioned for LangGraph Platform are:\\n\\n1. Self-Hosted Lite: A free, limited version that you can run locally or in a self-hosted manner.\\n2. Cloud SaaS: Hosted as part of LangSmith.\\n3. Bring Your Own Cloud: Infrastructure is managed by LangGraph but runs within your cloud.\\n4. Self-Hosted Enterprise: Completely managed by you.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith has two usage limits: total traces and extended',\n",
       " 'context': [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content=\"Deployment Options¶\\n\\nSelf-Hosted Lite: A free (up to 1 million nodes executed), limited version of LangGraph Platform that you can run locally or in a self-hosted manner\\nCloud SaaS: Hosted as part of LangSmith.\\nBring Your Own Cloud: We manage the infrastructure, so you don't have to, but the infrastructure all runs within your cloud.\\nSelf-Hosted Enterprise: Completely managed by you.\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                MULTIPLE_SUBGRAPHS\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Why LangGraph?\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\"),\n",
       "  Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content=\"Human-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.\\nPersistence: LangGraph has a built-in persistence layer, implemented through checkpointers. This persistence layer helps to support powerful capabilities like human-in-the-loop, memory, time travel, and fault-tolerance.\\nMemory: Memory in AI applications refers to the ability to process, store, and effectively recall information from past interactions. With memory, your agents can learn from feedback and adapt to users' preferences.  \\nStreaming: Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs. \\nFAQ: Frequently asked questions about LangGraph.\"),\n",
       "  Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Glossary: LangGraph workflows are designed as graphs, with nodes representing different components and edges representing the flow of information between them. This guide provides an overview of the key concepts associated with LangGraph graph primitives.\\nCommon Agentic Patterns: An agent uses an LLM to pick its own control flow to solve more complex problems! Agents are a key building block in many LLM applications. This guide explains the different types of agent architectures and how they can be used to control the flow of an application.\\nMulti-Agent Systems: Complex LLM applications can often be broken down into multiple agents, each responsible for a different part of the application. This guide explains common patterns for building multi-agent systems.\\nHuman-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.'),\n",
       "  Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Server¶\\n\\nApplication Structure: A LangGraph application consists of one or more graphs, a LangGraph API Configuration file (langgraph.json), a file that specifies dependencies, and environment variables.\\nAssistants: Assistants are a way to save and manage different configurations of your LangGraph applications.\\nWeb-hooks: Webhooks allow your running LangGraph application to send data to external services on specific events.\\nCron Jobs: Cron jobs are a way to schedule tasks to run at specific times in your LangGraph application.\\nDouble Texting: Double texting is a common issue in LLM applications where users may send multiple messages before the graph has finished running. This guide explains how to handle double texting with LangGraph Deploy.\\n\\nDeployment Options¶')],\n",
       " 'answer': 'What are the deployment options mentioned for LangGraph Platform based on the provided context?\\n\\nThe deployment options mentioned for LangGraph Platform are:\\n\\n1. Self-Hosted Lite: A free, limited version that you can run locally or in a self-hosted manner.\\n2. Cloud SaaS: Hosted as part of LangSmith.\\n3. Bring Your Own Cloud: Infrastructure is managed by LangGraph but runs within your cloud.\\n4. Self-Hosted Enterprise: Completely managed by you.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content=\"Deployment Options¶\\n\\nSelf-Hosted Lite: A free (up to 1 million nodes executed), limited version of LangGraph Platform that you can run locally or in a self-hosted manner\\nCloud SaaS: Hosted as part of LangSmith.\\nBring Your Own Cloud: We manage the infrastructure, so you don't have to, but the infrastructure all runs within your cloud.\\nSelf-Hosted Enterprise: Completely managed by you.\\n\\nComments\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                MULTIPLE_SUBGRAPHS\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Why LangGraph?\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\"),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content=\"Human-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.\\nPersistence: LangGraph has a built-in persistence layer, implemented through checkpointers. This persistence layer helps to support powerful capabilities like human-in-the-loop, memory, time travel, and fault-tolerance.\\nMemory: Memory in AI applications refers to the ability to process, store, and effectively recall information from past interactions. With memory, your agents can learn from feedback and adapt to users' preferences.  \\nStreaming: Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs. \\nFAQ: Frequently asked questions about LangGraph.\"),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Glossary: LangGraph workflows are designed as graphs, with nodes representing different components and edges representing the flow of information between them. This guide provides an overview of the key concepts associated with LangGraph graph primitives.\\nCommon Agentic Patterns: An agent uses an LLM to pick its own control flow to solve more complex problems! Agents are a key building block in many LLM applications. This guide explains the different types of agent architectures and how they can be used to control the flow of an application.\\nMulti-Agent Systems: Complex LLM applications can often be broken down into multiple agents, each responsible for a different part of the application. This guide explains common patterns for building multi-agent systems.\\nHuman-in-the-Loop: Explains different ways of integrating human feedback into a LangGraph application.'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/#langgraph', 'title': 'Concepts', 'description': 'Conceptual Guide for LangGraph', 'language': 'en'}, page_content='LangGraph Server¶\\n\\nApplication Structure: A LangGraph application consists of one or more graphs, a LangGraph API Configuration file (langgraph.json), a file that specifies dependencies, and environment variables.\\nAssistants: Assistants are a way to save and manage different configurations of your LangGraph applications.\\nWeb-hooks: Webhooks allow your running LangGraph application to send data to external services on specific events.\\nCron Jobs: Cron jobs are a way to schedule tasks to run at specific times in your LangGraph application.\\nDouble Texting: Double texting is a common issue in LLM applications where users may send multiple messages before the graph has finished running. This guide explains how to handle double texting with LangGraph Deploy.\\n\\nDeployment Options¶')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
